{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Working with text data\n",
    "\n",
    "文本是最常见的序列数据之一，一般理解为单词序列。由于深度学习模型并非接收原始文本作为输入，它只能处理数值张量。因此要先将文本向量化vectorize，一般有以下几种实现方法：\n",
    "\n",
    "- 将文本分割为单词，并将每个单词转换为一个向量\n",
    "- 将文本分割为字符，并将每个字符转换为一个向量\n",
    "- 提取单词或字符的n-gram，并将每个n-gram转换为一个向量。n-gram是多个相邻单词或字符的集合，n-gram之间可以重叠。\n",
    "\n",
    "将文本分解而成的单元(单词、字符或n-gram)叫作token，将文本分解为标记的过程叫作分词(tokenization)。所有文本向量化过程都是**应用某种分词方案，然后将数值向量与生成的标记相关联。**本接介绍两种方法，one-hot encoding与token embedding。\n",
    "\n",
    "n-gram的词袋模型是一种特征工程工具，不适用于深度学习模型，但对于轻量级的浅层文本处理模型(如logistic回归和random forest)是重要且不可或缺的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.1 One-hot encoding of words and characters\n",
    "\n",
    "one-hot编码是将token转换为向量最基本的方法。它将每个单词和一个唯一的整数index相关联，并将这个整数index转换为长度为N的二进制向量(N是词表大小)。\n",
    "\n",
    "下面是单词级的one-hot编码和字符级的one-hot编码的简单示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "\n",
    "token_index = {}\n",
    "for sample in samples:\n",
    "    for word in sample.split():\n",
    "        if word not in token_index:\n",
    "            token_index[word] = len(token_index) + 1\n",
    "\n",
    "max_length = 10\n",
    "\n",
    "results = np.zeros(shape=(len(samples),\n",
    "                          max_length,\n",
    "                          max(token_index.values())+ 1))\n",
    "\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "        index = token_index.get(word)\n",
    "        results[i, j, index] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras中提供了内置函数，其可以对原始文本数据进行单词级或字符级的one-hot编码。这些函数一般实现了许多重要特性，应该使用它们对文本数据进行编码处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# 6-3 Using Keras for word-level one-hot encoding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "tokenizer.fit_on_texts(samples)\n",
    "\n",
    "# to list\n",
    "sequences = tokenizer.texts_to_sequences(samples)\n",
    "\n",
    "# to one-hot tensor\n",
    "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one-hot encoding的一种变体是one-hot hashing trick，适用于词表中token数目太大而无法直接处理的情况。这种方法没有为每个单词显示分配一个index，而是将单词hash encode为固定长度的向量，这一步通常用一个简单的hash function实现。优点在于避免维护一个显式的单词索引，从而能够节省内存并允许数据的online encoding(在读取完所有数据前即可生成encoding)；缺点在于可能出现hash collision。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.2 Using word embeddings\n",
    "\n",
    "word embedding相比于one-hot编码的词向量，具有以下特征：\n",
    "- one-hot词向量稀疏、高维、hard-code\n",
    "- word embedding密集、低维且从数据中学习得到\n",
    "\n",
    "word embedding的获取有两种方法：\n",
    "- 在完成主任务的同时学习该任务上的embedding。\n",
    "- 在不同于待解决问题的机器学习任务上预先计算好word embedding，然后将其加载到模型中，这种word embedding叫作pretrained word embedding。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Learning word embedding with Embedding layer\n",
    "\n",
    "词向量之间的几何关系应该表示这些词之间的语义关系。也即word embedding的作用应该是将人类的语言映射到几何空间中。词与词之间的distance和direction应该都是meaningful的。\n",
    "\n",
    "对于一特定任务如需学习一个其word embeddings对应的嵌入空间，意味着要学习一个层的weights，这个层也即是Embedding层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(1000, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding层至少需要两个参数，input_dim可理解为token个数，output_dim可理解为embedding的维度。这样一来，Embedding层就可以被视为一个字典，它将单词的整数索引映射为密集向量，实际上就是一种字典查找。\n",
    "\n",
    "这个Embedding层的input应该是一个shape为(samples, sequence_length)的张量，其中sequence_length用zero padding等方法做到长度一致。而output的shape应该是(samples, sequence_length, embedding_dimensionality)，之后可以用RNN层或者1D conv层来处理这个三维张量。\n",
    "\n",
    "下面在IMDB的电影评论数据集上训练一个word embedding，将评论长度限制为只有20个单词，词表选择前10000个单词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras import preprocessing\n",
    "\n",
    "max_features = 10000\n",
    "maxlen = 20\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=\n",
    "                                                     max_features)\n",
    "\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 20, 8)             80000     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 161       \n",
      "=================================================================\n",
      "Total params: 80,161\n",
      "Trainable params: 80,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 36s 2ms/step - loss: 0.6759 - acc: 0.6050 - val_loss: 0.6398 - val_acc: 0.6814\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 8s 415us/step - loss: 0.5657 - acc: 0.7427 - val_loss: 0.5467 - val_acc: 0.7206\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 9s 432us/step - loss: 0.4752 - acc: 0.7808 - val_loss: 0.5113 - val_acc: 0.7384\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 8s 403us/step - loss: 0.4263 - acc: 0.8077 - val_loss: 0.5008 - val_acc: 0.7452\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 8s 423us/step - loss: 0.3930 - acc: 0.8258 - val_loss: 0.4981 - val_acc: 0.7538\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 9s 427us/step - loss: 0.3668 - acc: 0.8395 - val_loss: 0.5014 - val_acc: 0.7530\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 9s 425us/step - loss: 0.3435 - acc: 0.8533 - val_loss: 0.5052 - val_acc: 0.7520446 - acc: 0.8\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 8s 414us/step - loss: 0.3223 - acc: 0.8657 - val_loss: 0.5132 - val_acc: 0.7486\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 8s 420us/step - loss: 0.3022 - acc: 0.8766 - val_loss: 0.5213 - val_acc: 0.7490\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 8s 424us/step - loss: 0.2839 - acc: 0.8860 - val_loss: 0.5303 - val_acc: 0.7466\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Embedding\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 8, input_length=maxlen))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10, batch_size=32,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到最后训练出的Embedding层的表示用于二分类任务的分类精度约为75%。但是用Dense层而非序列模型的RNN unit作分类会导致模型对输入序列中的每个单词单独处理，而并没有考虑单词之间的关系和句子结构。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using pretrained word embedding\n",
    "\n",
    "当手头可用的训练数据很少时，可从pretrained的embedding空间加载word embedding。通常这个pretrained embedding space是高度结构化的，且抓住了语言结构的一般特点。这样可以不必在解决问题的同时学习word embedding。\n",
    "\n",
    "有许多pretrained的word embedding，都可以下载并在Keras的Embedding层中使用，如word2vec和GloVe。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.3 Putting it all together: from raw text to word embeddings\n",
    "\n",
    "本节中将使用GloVe作为embedding实现IMDB数据集上的情感分类，其中IMDB的数据将使用IMDB的原始文本数据，而不是Keras内置的已经预先分词的IMDB数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "imdb_dir = r''\n",
    "train_dir = r''\n",
    "\n",
    "labels = []\n",
    "texts = []\n",
    "\n",
    "for label_type in ['pos', 'neg']:\n",
    "    dir_name = os.path.join()\n",
    "    for fname in os.listdir(dir_name):\n",
    "        if fname[-4:] == '.txt':\n",
    "            f = open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
